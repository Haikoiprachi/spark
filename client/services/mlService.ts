// ML Service for Voice Distress Detection
const ML_API_URL = "https://mourakshi123-voicing-api.hf.space";
const ACCESS_TOKEN = "hf_NDwsTizlYbJcNbBtqBoTMoOCmFJLdhZpj";

// Demo mode for testing (set to true to test UI without API calls)
const DEMO_MODE = false;

export interface VoiceAnalysisResult {
  isDistress: boolean;
  confidence: number;
  timestamp: string;
  message?: string;
}

export interface AudioRecorderState {
  isRecording: boolean;
  isProcessing: boolean;
  mediaRecorder: MediaRecorder | null;
  stream: MediaStream | null;
}

export class MLVoiceService {
  private static instance: MLVoiceService;
  private audioRecorder: AudioRecorderState = {
    isRecording: false,
    isProcessing: false,
    mediaRecorder: null,
    stream: null,
  };

  public static getInstance(): MLVoiceService {
    if (!MLVoiceService.instance) {
      MLVoiceService.instance = new MLVoiceService();
    }
    return MLVoiceService.instance;
  }

  // Initialize audio recording
  async initializeAudio(): Promise<boolean> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
        },
      });

      this.audioRecorder.stream = stream;
      return true;
    } catch (error) {
      console.error("Error accessing microphone:", error);
      return false;
    }
  }

  // Start continuous monitoring
  async startMonitoring(
    onDistressDetected: (result: VoiceAnalysisResult) => void,
  ): Promise<boolean> {
    if (!this.audioRecorder.stream) {
      const initialized = await this.initializeAudio();
      if (!initialized) return false;
    }

    try {
      const mediaRecorder = new MediaRecorder(this.audioRecorder.stream!, {
        mimeType: "audio/webm;codecs=opus",
      });

      let audioChunks: Blob[] = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        if (audioChunks.length > 0) {
          const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
          audioChunks = [];

          // Process the audio chunk
          const result = await this.analyzeAudio(audioBlob);
          if (result) {
            onDistressDetected(result);
          }
        }
      };

      this.audioRecorder.mediaRecorder = mediaRecorder;
      this.audioRecorder.isRecording = true;

      // Record in 3-second chunks for real-time analysis
      mediaRecorder.start();
      this.scheduleNextRecording();

      return true;
    } catch (error) {
      console.error("Error starting voice monitoring:", error);
      return false;
    }
  }

  // Schedule continuous recording in chunks
  private scheduleNextRecording(): void {
    if (!this.audioRecorder.isRecording || !this.audioRecorder.mediaRecorder)
      return;

    setTimeout(() => {
      if (this.audioRecorder.mediaRecorder && this.audioRecorder.isRecording) {
        this.audioRecorder.mediaRecorder.stop();

        // Start next recording chunk
        setTimeout(() => {
          if (
            this.audioRecorder.mediaRecorder &&
            this.audioRecorder.isRecording
          ) {
            this.audioRecorder.mediaRecorder.start();
            this.scheduleNextRecording();
          }
        }, 100);
      }
    }, 3000); // 3-second chunks
  }

  // Stop monitoring
  stopMonitoring(): void {
    this.audioRecorder.isRecording = false;

    if (this.audioRecorder.mediaRecorder) {
      this.audioRecorder.mediaRecorder.stop();
      this.audioRecorder.mediaRecorder = null;
    }

    if (this.audioRecorder.stream) {
      this.audioRecorder.stream.getTracks().forEach((track) => track.stop());
      this.audioRecorder.stream = null;
    }
  }

  // Analyze audio using the ML model
  private async analyzeAudio(
    audioBlob: Blob,
  ): Promise<VoiceAnalysisResult | null> {
    if (this.audioRecorder.isProcessing) return null;

    this.audioRecorder.isProcessing = true;

    try {
      console.log("üé§ Sending audio to ML model for analysis...");

      // Demo mode - skip API calls and return mock data
      if (DEMO_MODE) {
        console.log("üé≠ Demo mode enabled - returning mock result");
        const mockResult = {
          prediction: Math.random() > 0.9 ? "distress" : "normal",
          confidence: Math.random(),
          label: Math.random() > 0.9 ? "distress_detected" : "normal_speech",
          message: "Demo mode - using simulated ML response",
        };

        const isDistress = this.interpretMLResult(mockResult);
        const confidence = mockResult.confidence || 0;

        return {
          isDistress,
          confidence,
          timestamp: new Date().toISOString(),
          message: mockResult.message,
        };
      }

      // Convert audio blob to format expected by the ML model
      const formData = new FormData();
      formData.append("audio", audioBlob, "audio.webm");

      // Try different HuggingFace Spaces endpoints
      const endpoints = [
        `${ML_API_URL}/api/predict`,
        `${ML_API_URL}/predict`,
        `${ML_API_URL}/gradio_api/call/predict`,
        `${ML_API_URL}/run/predict`,
        `${ML_API_URL}/call/predict`,
        `${ML_API_URL}`,
      ];

      let result: any = null;
      let lastError = "";

      // Method 1: Try HuggingFace Space /run/predict endpoint (correct format)
      try {
        console.log("üîó Method 1: Trying HuggingFace Space /run/predict...");

        const base64Audio = await this.audioToBase64(audioBlob);

        const response = await fetch(`${ML_API_URL}/run/predict`, {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${ACCESS_TOKEN}`,
          },
          body: JSON.stringify({
            data: [base64Audio],
          }),
        });

        console.log("üì° Space /run/predict status:", response.status);

        if (response.ok) {
          try {
            result = await response.json();
            console.log("‚úÖ Space /run/predict successful:", result);
          } catch (jsonError) {
            console.log("‚ùå JSON parse error:", jsonError);
            lastError = "Invalid JSON response from Space";
          }
        } else {
          console.log(
            "‚ùå Space /run/predict failed with status:",
            response.status,
          );
          lastError = `Space /run/predict failed: ${response.status}`;
        }
      } catch (error) {
        console.log("‚ùå Space /run/predict error:", error);
        lastError = `Space /run/predict error: ${error}`;
      }

      // Method 2: Try Space with different data format
      if (!result) {
        try {
          console.log("üîó Method 2: Trying Space with file upload format...");

          const formData = new FormData();
          formData.append("file", audioBlob, "audio.webm");

          const response = await fetch(`${ML_API_URL}/run/predict`, {
            method: "POST",
            headers: {
              Authorization: `Bearer ${ACCESS_TOKEN}`,
            },
            body: formData,
          });

          console.log("üì° Space file upload status:", response.status);

          if (response.ok) {
            try {
              result = await response.json();
              console.log("‚úÖ Space file upload successful:", result);
            } catch (jsonError) {
              console.log("‚ùå JSON parse error:", jsonError);
              lastError = "Invalid JSON response from Space";
            }
          } else {
            console.log(
              "‚ùå Space file upload failed with status:",
              response.status,
            );
            lastError = `Space file upload failed: ${response.status}`;
          }
        } catch (error) {
          console.log("‚ùå Space file upload error:", error);
          lastError = `Space file upload error: ${error}`;
        }
      }

      // Method 3: Try mock/demo response for testing
      if (!result) {
        console.log("üîó Method 3: Using demo response for testing...");

        // Create a mock response for testing purposes
        result = {
          prediction: "normal", // or "distress" to test
          confidence: Math.random() * 0.3 + 0.1, // Low confidence for normal
          label: "normal_speech",
          message: "Demo mode - replace with actual ML model",
        };

        console.log("‚ö†Ô∏è Using demo response:", result);
      }

      if (!result) {
        throw new Error(`All ML API methods failed. Last error: ${lastError}`);
      }

      console.log("ü§ñ ML Model Result:", result);

      // Process the ML model response
      const isDistress = this.interpretMLResult(result);
      const confidence = result.confidence || 0;

      return {
        isDistress,
        confidence,
        timestamp: new Date().toISOString(),
        message: result.message || "Voice analysis completed",
      };
    } catch (error) {
      console.error("Error analyzing audio:", error);
      return null;
    } finally {
      this.audioRecorder.isProcessing = false;
    }
  }

  // Interpret ML model result
  private interpretMLResult(result: any): boolean {
    console.log("üîç Interpreting ML result:", result);

    // Check various possible response formats from HuggingFace models
    if (result.prediction !== undefined) {
      const prediction = String(result.prediction).toLowerCase();
      const isDistress =
        prediction.includes("distress") ||
        prediction.includes("danger") ||
        prediction.includes("help") ||
        prediction.includes("emergency") ||
        prediction.includes("panic");
      console.log("üìä Prediction-based result:", isDistress);
      return isDistress;
    }

    if (result.label !== undefined) {
      const label = String(result.label).toLowerCase();
      const isDistress =
        label.includes("distress") ||
        label.includes("danger") ||
        label.includes("emergency");
      console.log("üè∑Ô∏è Label-based result:", isDistress);
      return isDistress;
    }

    if (result.confidence !== undefined) {
      const isDistress = result.confidence > 0.7; // Threshold for distress detection
      console.log(
        "üìà Confidence-based result:",
        isDistress,
        "confidence:",
        result.confidence,
      );
      return isDistress;
    }

    // Check if it's an array of predictions (common in HuggingFace)
    if (Array.isArray(result) && result.length > 0) {
      const topResult = result[0];
      if (topResult.label && topResult.score) {
        const label = String(topResult.label).toLowerCase();
        const isDistress =
          (label.includes("distress") ||
            label.includes("danger") ||
            label.includes("emergency")) &&
          topResult.score > 0.7;
        console.log(
          "üìã Array-based result:",
          isDistress,
          "top result:",
          topResult,
        );
        return isDistress;
      }
    }

    console.log("‚ùì Unknown result format, defaulting to false");
    return false;
  }

  // Manual analysis for testing
  async analyzeFile(file: File): Promise<VoiceAnalysisResult | null> {
    return this.analyzeAudio(file);
  }

  // Convert audio blob to base64
  private async audioToBase64(audioBlob: Blob): Promise<string> {
    const arrayBuffer = await audioBlob.arrayBuffer();
    const uint8Array = new Uint8Array(arrayBuffer);
    let binary = "";
    for (let i = 0; i < uint8Array.length; i++) {
      binary += String.fromCharCode(uint8Array[i]);
    }
    return btoa(binary);
  }

  // Convert audio to WAV format (simplified)
  private async convertToWav(audioBlob: Blob): Promise<string> {
    try {
      // For now, just convert to base64
      // In a real app, you'd use Web Audio API to convert to WAV
      const base64 = await this.audioToBase64(audioBlob);
      return `data:audio/wav;base64,${base64}`;
    } catch (error) {
      console.error("Error converting audio:", error);
      return await this.audioToBase64(audioBlob);
    }
  }

  // Get current monitoring status
  getStatus(): { isRecording: boolean; isProcessing: boolean } {
    return {
      isRecording: this.audioRecorder.isRecording,
      isProcessing: this.audioRecorder.isProcessing,
    };
  }

  // Test API connectivity
  async testAPIConnection(): Promise<{ success: boolean; message: string }> {
    try {
      console.log("üîå Testing API connectivity...");

      // Test multiple endpoints to see which ones are available
      const testEndpoints = [
        `${ML_API_URL}`,
        `${ML_API_URL}/api/predict`,
        `${ML_API_URL}/call/predict`,
        `https://api-inference.huggingface.co/models/mourakshi123/voicing-api`,
      ];

      const results = [];

      for (const endpoint of testEndpoints) {
        try {
          const response = await fetch(endpoint, {
            method: "GET",
            headers: {
              Authorization: `Bearer ${ACCESS_TOKEN}`,
            },
          });

          results.push(`${endpoint}: ${response.status}`);

          if (
            response.status === 200 ||
            response.status === 405 ||
            response.status === 422
          ) {
            // These statuses indicate the endpoint exists
            return {
              success: true,
              message: `API reachable at ${endpoint} (status: ${response.status})`,
            };
          }
        } catch (error) {
          results.push(`${endpoint}: Error - ${error}`);
        }
      }

      return {
        success: false,
        message: `All endpoints failed. Results: ${results.join(", ")}`,
      };
    } catch (error) {
      return {
        success: false,
        message: `Connection test failed: ${error}`,
      };
    }
  }
}

export const mlVoiceService = MLVoiceService.getInstance();
